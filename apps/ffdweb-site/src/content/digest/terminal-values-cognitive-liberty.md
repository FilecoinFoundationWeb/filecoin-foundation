---
title: "Terminal Values: Cognitive Liberty"
created-on: 2023-09-15T17:01:01.677Z
updated-on: 2023-09-15T17:01:01.686Z
published-on: 2023-09-15T17:01:01.693Z
f_issue: 1
f_article: 2
f_description: Your inalienable right to live rent-free inside your own head
f_image:
  url: /assets/images/DWD-Illustration-02.png
f_author: Danny O'Brien
f_author-profile:
  url: /assets/images/dwd-profile-02.png
f_author-bio: Danny O'Brien is Senior Fellow and DWeb Strategy at Filecoin
  Foundation and Filecoin Foundation for the Decentralized Web. He has been a
  tech journalist, developer and defender of online free speech, privacy and an
  open internet for more than 20 years, including over a decade's activism at
  the Electronic Frontier Foundation.
tags: digest
layout: "[digest].html"
date: 2023-09-15T17:01:01.699Z
seo:
  noindex: false
  twitter:title: ""
  twitter:card: ""
  title: "FFDW DWeb Digest: Terminal Values: Cognitive Liberty"
  og:image: /assets/images/DWD-Illustration-02.png
---
If you build and market new technologies to a global audience, you may occasionally reflect on how the use of those technologies align with your personal values and our collective human rights. Or you may not. Be warned, however: avoiding the topic entirely can lead to some uncomfortable situations down the line.

As an activist at the Committee to Protect Journalists and the Electronic Frontier Foundation, my job in the 2010s was to make sure those uncomfortable situations happened as early as possible in the product lifecycle. I was a sort of traveling conscience salesman, knocking on the door of shiny new start-ups like Tumblr, or fast-moving, thing-breaking giants like Facebook, and then shoving my foot in the door as I brandished my credentials. I learned that people in those companies mostly thought that human rights violations were something that happened far away, so I would sit with their development teams and ask if they knew how incredibly popular they were in another country – like Tunisia, say, or the Philippines. This made them happy.

Then I'd describe the kind of struggles human rights activists had in that place. They would look sad. Finally, I'd note some misfeature of their tooling that those activists had told me was screwing them over: how the sites' login page was entirely unencrypted, say, and was being intercepted by the government or other malicious actors holed up in that country's infrastructure.

That would usually make the team avoid eye contact with me entirely, but hopefully they would go back to their desks after my brown-bag talk and fix something. Anything. (The same misuses, vulnerabilities, and exploitations were happening under their noses in the United States, where they lived, but it would take a few more years before they would believe that.)

Since then, thanks to smarter activists than me from around the world, and more assiduous technologists at those companies, matters have improved. Your passwords are, I hope, encrypted in transit and at rest. Companies hear directly from those affected by their decisions around the world, as well as in their own home country. There is a far richer conversation across society on the ethical deployment of digital technology.

But the reflections and doubts we struggle with have grown more complex, more dialectical even.

The psychologist Milton Rokeach contrasted the deeper goals of culture, which he termed "terminal values," with the methods we use to implement and maintain them, which were "instrumental values.”

In those more naive times, the human rights I would tout would be blunt and absolute: defend free speech, protect privacy. Now, even digital rights activists collectively wonder: are those really our terminal values? Or do we ask these big tech companies to do these things in the pursuit of wider, more fundamental values?

Perhaps we don't want our technology to be an engine of unbounded free expression, and unstoppable privacy. Perhaps we hold those values contingent on their capability to help us to achieve a more democratic society, or social equity or stability or prosperity or safety.

After seeing up close what poor a job those tech giants have made of defending frankly any set of consistent values, fundamental or not, I've turned to a new job at the Filecoin Foundation for the Decentralized Web, where I work to bolster the ability of Internet users to create and use decentralized alternatives to those weary tech giants.

I sincerely believe decentralization can lead to better protections for the values and rights that we hold in common. But, as I foster and create and brandish these new technologies, I've found myself pausing to reflect too. Is decentralization a terminal value?

If decentralizing tech — and distributing its powers more widely — fails to serve our more fundamental needs, should we fall back to those giant centralized systems, imperfect as they may be? Should we even hold back from supporting such wild new technologies, given where so many people believe the last wave of digital tech led us?

I do believe there are more fundamental digital values than speech, privacy, and decentralization. But there are not many, and they lie not so far from those needs. Rather than Rokeach's terminal values, which included "self-respect" and "inner harmony," and to which we might add such clear and pressing concerns as the fight against racism, poverty, and injustice, I think there is one fundamental terminal value that these digital rights ultimately — and intimately — defend and enhance.

There's no established name that I know of for this concept that will bring it instantly to mind. The right of self-determination, from the human rights tradition, cuts close. Duke University's Nita A. Farahany’s recent re-coinage of the term "cognitive liberty" in her new book "The Battle for Your Brain: Defending Your Right to Think Freely in the Age of Neurotechnology" is a brilliant framing and naming, and deserves to be widely adopted, especially given the dystopian technological applications she documents.

But my route to this terminal value is a little different, and comes from an older, more optimistic tech tradition: one that still lies, sometimes deeply buried, behind the screens we use today.

The "PC" that perhaps still sits near you somewhere, if it hasn't shrunk into your laptop or your phone, has always stood for "personal computer". That name is an echo of a line of revolutionary 20th century thought; a profound ideological rebellion from the locked-down, timesharing and centralized mainframe ideologies that preceded it.

The PC was always intended as a machine that augments individual abilities. That ambition has deep roots, from Vannevar Bush's 1945 essay "[As We May Think](https://www.theatlantic.com/magazine/archive/1945/07/as-we-may-think/303881/)," Doug Engelbart's 1962 paper "[Augmenting Human Intellect](https://www.dougengelbart.org/pubs/augment-3906.html)," through Ted Nelson's 1974 manifesto "[Computer Lib](http://www.thetednelson.com/computerlibanddreammachines.php)," Steve Job's 1980 "Bicycle For The Mind" campaign, to Sherry Turkle's 1984 book "The Second Self" and beyond.

In this way of thinking about digital tech, the personal computer is an extension of your brain and its abilities. Its memory is to help you remember; its processing power is there to help you think faster; its network connection is for you to reach out to others; its interfaces are to connect more closely to you. It is yours in the same way as your hands belong to you, as your eyes, as your imagination.

Something has taken us from that tradition. The PC has inched closer to our faces, and under our skin. It has become ever more personal and intimate (do you sleep with your phone?) It has in many ways become more "user friendly." But it has also become much much less user controlled. Its memory and processor now spends its time on showing advertisements, enforcing copyright protection rules, and conducting sly surveillance of your habits, using systems that resist your ability to evade them. That network connection is used to stream out your behavior to strangers, rather than let you voluntarily choose with whom you communicate and what about.

No matter how they ape the liberatory language of this tradition, many of us look at Neuralink or VR and see it as a fundamentally alienating tech, controlled by others, leering into our personal space; foreign body horror rather than extensions of ourselves.

Those on the cutting edge of technological adoption, like elderly or disabled people, know the profound difference between intimate tech that expands your personal autonomy and that which is limited and controlled by others. Many others who might think they have more freedom in what tech they adopt are feeling the walls close in too.

Farahany captures this growing risk in her book: of technology used to spy into your brain, or even worse, to reach in and manipulate it. We can understand from her examples that this is the minimum freedom we need if we are even to be able to address or defend or experience all those other terminal values. We can't fight injustice, we can't even see injustice, if we can't think about it. We can't see alternatives to the way we live if we do not have control of our own thoughts, our conscience, our free will. We can't revel in a free world, when our minds are in shackles.

But marking the perimeter of our freedom around our skulls is to mislay the potential alternative. Personal technology can be an extension of our mind. With personal computers of all shapes, under our conscious control, acting as our faithful agents, we can think faster, consider more options, grow and work collectively and in a more fulfilling way than was possible than ever before.

It's more than a goal of technology, it's the only version that leads to a positive vision of rights and values, as opposed to a slowly more closed-in, limited world.

The personal technology that forms our "exocortex," as blogger Ben Houston called it, must  possess the same free-wheeling, unbounded liberty as exists inside our heads.

This is why free expression and privacy are so fundamental in the world of technology, perhaps even more than in their historical context. It's not just about being able to say anything to anyone in the public square, or even keeping your messages to others private. It's about being able to speak privately to ourselves.

When I talk about decentralizing tech, what I most think about is moving processing, storage and control back toward the edge, back closer to the end-user and their control.

Right now, if you wrote a note to yourself and put it on your personal computer's hard drive, you could say whatever you want, draw whatever you want. If it's in the cloud, you don't have that guarantee: even if you kept it in your own Google Drive or DropBox, it could be found wanting, and deleted. I don't know about you, but my notes are my memory, most of the time. My search engine searches are as much me talking to myself about my worries and interests, in a way that I would rarely disclose even to my closest companion.

And of course, AI conversations are becoming even more tightly linked to our explorations, our reflections, the intermediate steps of our imagination.

Ultimately, as a digital rights activist back in the 2010s, what I was doing was demeaning to me and to those I sought, clumsily, to represent. I went on bended knee and begged the indulgence of centralized services, hoping to catch their sympathy or provoke their sense of shame, to extract a few temporary concessions that could make the tools they made a little bit more aligned with the desperate needs of their captive userbase.

That's no way to defend inalienable rights. And those rights need to be more than defended: they need to expand to fit the challenges we now face. As Engelbart wrote, "\[t]he complexity of the problems facing mankind is growing faster than our ability to solve them." As individuals and collectively, we need our own abilities to grow to match the challenges of the modern world.

The place where everything about human nature starts, and ends, is within our own consciousness. Personal computers give us the chance to expand that consciousness; but that means we need to expand the perimeter of our basic freedom to think.

Our own consciousness cannot be rented from others, or temporarily conceded to us, with built-in police or backdoors or hidden ad men. We need to seize the means of computation, and that means ejecting all of these interlopers, and relocating it back into the personal domain we control: whether that's physically, or by using tools like encryption and zero-knowledge proofs to preserve our control when our data and processing power sits on others' hardware.

That's the pyramid of digital rights for me: a firm foundation of decentralized, user-controlled technology, giving us broader cognitive liberty, internal privacy, freedom of self-expression, and freedom of self-determination. On top of that solid ground, we can build a society that's free and fair. And then we can have the ability and freedom to self-reflect, to talk, and to plot our better shared future together, free at last in our digital environment.