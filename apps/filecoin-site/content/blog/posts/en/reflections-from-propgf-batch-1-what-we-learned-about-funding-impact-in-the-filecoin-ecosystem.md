---
title: >-
  Reflections from ProPGF Batch 1: What We Learned About Funding Impact in the
  Filecoin Ecosystem
draft: false
excerpt: >-
  ProPGF Batch 1 funded 14 Filecoin ecosystem teams with $3.68M, testing a
  transparent network-led funding model and revealing key lessons for a more
  efficient, data-driven Batch 2.
share_image: /uploads/101725_ProPGF_Learning_Reflections.webp
image:
  url: /uploads/101725_ProPGF_Learning_Reflections.webp
author: Sejal Rekhan & Josh Daniels
date: 2025-10-17T17:54:03.545Z
categories:
  - updates
dim_image: true
---

## A Prototype for Network-Led Public Goods Funding

When we [launched](https://filecoin.io/blog/posts/introducing-fil-propgf-a-new-era-in-community-led-public-goods-funding-for-the-filecoin-ecosystem/) Filecoin ProPGF in early 2025, our goal was simple but ambitious: to prototype a transparent, community-driven, and repeatable funding mechanism for the teams  advancing the Filecoin network.

Batch 1 was our first real test. Over the last few months, we reviewed 38 applications, shortlisted 23, and [funded 14 teams with a total of $3.68 million](https://filecoin.io/blog/posts/filecoin-propgf-batch-1-36m-awarded-to-ecosystem-builders/).

The round was guided by a [12-member Selection Committee](https://filecoin.io/blog/posts/announcing-the-propgf-selection-committee-members/)representing diverse expertise across the Filecoin ecosystem, using Questbook for grant management.

This first batch wasn’t just about distributing funds; it was about learning how a decentralized network can coordinate experts, funders, and builders toward shared priorities. What follows is a reflection on what worked, what didn’t, and what we’re taking forward.

## What Worked

Diverse, high-impact projects.
Batch 1 funded a wide cross-section of the ecosystem from infrastructure and developer tooling to retrieval networks and economic research. Teams like IPNI, FIL-B, FilPonto, CID Gravity, Curio Storage, and others are already pushing the boundaries of what’s possible on Filecoin.

Engaged reviewers and clear accountability.
The Selection Committee brought deep technical context and a willingness to dive in. Our milestone-based structure added rigor, helping align payouts with progress and building trust between teams and reviewers.

Transparency and collaboration.
Open calls and shared documentation created a culture of transparency. Several teams said it was the first time they could see how and why funding decisions were made.

## Where We Struggled (Project Perspective)

Our most valuable lessons came from project feedback.

Operational friction.
Delays in disbursements, fragmented communication, and confusing KYB/KYC steps made the process more stressful than it needed to be. Some teams had to cover costs out-of-pocket while waiting for payouts – which were reimbursed via the program.

Process duplication.
Projects often had to rewrite the same milestones or roadmap in multiple formats, while their specific scopes evolved mid-process. The lack of a single source of truth created uncertainty.

Tooling gaps.
Our third party grants management platform, while functional, wasn’t flexible enough for real-time collaboration. Teams asked for a unified dashboard to track milestone approvals, payments, and required documentation.

Clarity and predictability.
Grantees want to know exactly when reviews and payouts will happen, who’s responsible at each stage, and what documents are needed. These are fair expectations and they’re top of our fix list.

## What Reviewers Taught Us

Ambiguous goals and unclear criteria.
Several reviewers shared that the round’s goals like supporting innovation, retaining teams, and funding ecosystem stability weren’t clearly prioritized. This made evaluations subjective and occasionally inconsistent.

Time and scope.
Reviewers invested far more time than anticipated often 10 to 60 hours in total. Future cycles will provide clear expectations, scheduling support, and earlier notice.

Structure and metrics.
Many proposals lacked measurable outcomes or post-grant plans. We’ll now require success metrics tied to major milestones, and ecosystem-fit rationales upfront.

Ecosystem alignment.
Reviewers emphasized the need for a shared document outlining Filecoin’s network priorities to guide funding focus. This will be developed collaboratively in future rounds.

Tooling and format.
Suggestions included demo videos, standardized rubrics, and better systems for commenting and scoring—simple upgrades that dramatically improve clarity.

## Internal Reflections

Running this prototype also affirmed how much operational design and coordination matter in building effective funding systems at scale. We also saw how fragmented communication channels created confusion. Moving forward, there will be a single, reliable channel for updates, supported by regular reminders and touchpoints to keep everyone in sync.

Another key learning was that proposal reviewers and milestone reviewers often need different kinds of expertise. Batch 2 will therefore introduce an ecosystem call for milestone reviewers with the technical depth to assess implementation quality on a rolling basis.

Finally, we recognized the need for stronger post-funding evaluation. Both ProPGF and RetroPGF have so far relied on narrative reports mostly, which don’t fully capture ecosystem value or return on investment. The next phase will introduce a shared metrics framework and dashboard to track progress over time.

These reflections also reminded us of something simpler but equally important: communication and clarity. Teams need to know when rounds open and close, how to qualify, and what the program’s long-term vision is. Establishing predictability will make the system easier to navigate and build long-term trust in Filecoin’s funding mechanisms.

## Strategic Takeaways

1. From short-tail to long-tail funding. Batch 1 primarily funded near-term infrastructure; future rounds must also support exploratory project lead development across research, tokenomics, AI and funding mechanism design that keep Filecoin evolving.
2. Ecosystem coherence. ProPGF and RetroPGF should work in tandem and eventually connect to protocol-level funding streams.
3. Data-driven impact. Moving from narrative reports to measurable metrics will make funding more transparent and comparable.

## Looking Ahead - Batch 2 Timeline

Batch 2 launches next month, carrying forward these learnings into an improved and more predictable cycle.

![Batch 2 timeline showing applications open November–December 2025, review in January 2026, and KYC and payouts from February to March 2026](/uploads/screenshot-2025-10-17-at-1.55.24-PM.webp)

## Closing

Batch 1 proved that decentralized funding can work at scale but also that it requires discipline, empathy, and iteration. We’re proud of the teams, reviewers, and partners who made this prototype possible.

Each round brings us closer to a future where funding open infrastructure is as reliable and as evolving as the protocol itself.

Stay tuned for our next post, where we’ll share all the Batch 2 updates, timeline details, and improvements shaped by these learnings.
